{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ngrams based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "\u001b[32m2023-07-06 11:06:15.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mGot Data. Dataframe shape: (1464, 3)\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:15.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m229\u001b[0m - \u001b[1mUsing Elbow to define top ngram queries.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (280, 2)\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mMerged Ngrams. Dataframe shape: (243, 5)\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mFinal score and filter length: 243\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (243, 6)\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m233\u001b[0m - \u001b[1mGot samples. Number of samples: 243\u001b[0m\n",
      "\u001b[32m2023-07-06 11:06:16.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mUsing OpenAI API.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ngram_size           query  merged_frequency  frequency     score  \\\n",
      "0           1       investing          0.986842   0.625000  0.694132   \n",
      "1           1           hedge          0.039474   0.176471  0.553986   \n",
      "2           1     alternative          0.434211   1.000000  0.549265   \n",
      "3           2  private equity          1.000000   0.676471  0.452186   \n",
      "4           1      investment          0.355263   0.867647  0.444747   \n",
      "\n",
      "   search_volume  \n",
      "0       0.582343  \n",
      "1       1.000000  \n",
      "2       0.381424  \n",
      "3       0.066137  \n",
      "4       0.278040  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-06 11:08:48.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1mReviewing OpenAI's work.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:31.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m266\u001b[0m - \u001b[1mGetting structure.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:31.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m270\u001b[0m - \u001b[1mAdding categories.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:31.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m427\u001b[0m - \u001b[1mGetting embeddings.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:31.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "Batches: 100%|██████████| 23/23 [00:02<00:00,  9.44it/s]\n",
      "\u001b[32m2023-07-06 11:10:34.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_cluster_model\u001b[0m:\u001b[36m177\u001b[0m - \u001b[1mCluster Model: hdbscan\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:34.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_elbow\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mUsing epsilon value: 1.0536712127723509e-08\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:34.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m433\u001b[0m - \u001b[1mFitting model.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:34.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_reduced\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mReducing embeddings to 5 dims\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:51.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m436\u001b[0m - \u001b[1mInitial Model. Unique Labels: 156\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:51.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m441\u001b[0m - \u001b[1mRunning post processes for outliers.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:51.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m445\u001b[0m - \u001b[1mPost Processing. Unique Labels: 156\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:51.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m449\u001b[0m - \u001b[1mFinding names for cluster labels.\u001b[0m\n",
      "\u001b[32m2023-07-06 11:10:52.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 10.53it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcais_data.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m brand_terms \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcais\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcais group\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mglas\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mglas funds\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhalo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhalo investing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39micapital\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39micapital network\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m taxonomy, df, samples \u001b[39m=\u001b[39m create_taxonomy(filename,\n\u001b[0;32m      7\u001b[0m                                         text_column \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mkeyword\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                                         search_volume_column \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msearch_volume\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                                         taxonomy_model \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mopenai\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# \"palm\" or \"openai\"\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m                                         ngram_range \u001b[39m=\u001b[39;49m (\u001b[39m1\u001b[39;49m, \u001b[39m6\u001b[39;49m),\n\u001b[0;32m     11\u001b[0m                                         min_df \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m                                         brand_terms \u001b[39m=\u001b[39;49m brand_terms)\n\u001b[0;32m     14\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mtaxonomy.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(taxonomy))\n",
      "File \u001b[1;32mc:\\Users\\jroak\\Projects\\Taxonomy\\main.py:272\u001b[0m, in \u001b[0;36mcreate_taxonomy\u001b[1;34m(data, text_column, search_volume_column, taxonomy_model, use_clustering, use_llm_cluster_descriptions, cluster_embeddings_model, min_cluster_size, min_samples, days, ngram_range, min_df, brand_terms, limit_queries, debug_responses)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m# Add categories\u001b[39;00m\n\u001b[0;32m    270\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAdding categories.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 272\u001b[0m df \u001b[39m=\u001b[39m add_categories_clustered(structure, df, \n\u001b[0;32m    273\u001b[0m                               cluster_embeddings_model \u001b[39m=\u001b[39;49m cluster_embeddings_model,\n\u001b[0;32m    274\u001b[0m                               min_cluster_size \u001b[39m=\u001b[39;49m min_cluster_size,\n\u001b[0;32m    275\u001b[0m                               min_samples \u001b[39m=\u001b[39;49m min_samples)\n\u001b[0;32m    278\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDone.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m structure, df, samples\n",
      "File \u001b[1;32mc:\\Users\\jroak\\Projects\\Taxonomy\\main.py:306\u001b[0m, in \u001b[0;36madd_categories_clustered\u001b[1;34m(structure, df, cluster_embeddings_model, min_cluster_size, min_samples, match_col)\u001b[0m\n\u001b[0;32m    292\u001b[0m texts \u001b[39m=\u001b[39m df[match_col]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    294\u001b[0m model \u001b[39m=\u001b[39m ClusterTopics(\n\u001b[0;32m    295\u001b[0m         embedding_model \u001b[39m=\u001b[39m cluster_embeddings_model,\n\u001b[0;32m    296\u001b[0m         min_cluster_size \u001b[39m=\u001b[39m  min_cluster_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m         n_jobs \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m    303\u001b[0m     )\n\u001b[1;32m--> 306\u001b[0m labels, text_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(texts)\n\u001b[0;32m    307\u001b[0m label_lookup \u001b[39m=\u001b[39m {text: label \u001b[39mfor\u001b[39;00m text, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(texts, text_labels)}\n\u001b[0;32m    308\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtaxonomy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[match_col]\u001b[39m.\u001b[39mmap(label_lookup)\n",
      "File \u001b[1;32mc:\\Users\\jroak\\Projects\\Taxonomy\\lib\\clustering.py:454\u001b[0m, in \u001b[0;36mClusterTopics.fit\u001b[1;34m(self, corpus, top_n)\u001b[0m\n\u001b[0;32m    452\u001b[0m     label_mapping \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_text_label_mapping_llm(top_n\u001b[39m=\u001b[39mtop_n)\n\u001b[0;32m    453\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     label_mapping \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_text_label_mapping()\n\u001b[0;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_labels \u001b[39m=\u001b[39m [label_mapping[l] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels]\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_labels)\n",
      "File \u001b[1;32mc:\\Users\\jroak\\Projects\\Taxonomy\\lib\\clustering.py:320\u001b[0m, in \u001b[0;36mClusterTopics.get_text_label_mapping\u001b[1;34m(self, top_n)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(labels):\n\u001b[0;32m    319\u001b[0m         top_text_idx \u001b[39m=\u001b[39m neigh\u001b[39m.\u001b[39mkneighbors(np\u001b[39m.\u001b[39marray([text_label_embeddings[i]]))[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m--> 320\u001b[0m         text_labels[i] \u001b[39m=\u001b[39m categories[top_text_idx][\u001b[39m0\u001b[39m]\n\u001b[0;32m    323\u001b[0m mapping \u001b[39m=\u001b[39m {\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<outliers>\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    325\u001b[0m \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(labels):\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from main import create_taxonomy\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = False,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "df.to_csv(\"taxonomy.csv\", index=False)\n",
    "\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Description based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LLM descriptions of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy, add_categories\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = True,\n",
    "                                        use_llm_cluster_descriptions = True,\n",
    "                                        cluster_embeddings_model = \"local\", # \"palm\", \"openai\", or \"local\"\n",
    "                                        min_cluster_size = 5,\n",
    "                                        min_samples = 2,\n",
    "                                        use_llm_cluster_descriptions = True,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "df.to_csv(\"cais_data_taxonomy.csv\", index=False)\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using c-tfidf ngrams for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy, add_categories\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = True,\n",
    "                                        use_llm_cluster_descriptions = False,\n",
    "                                        cluster_embeddings_model = \"local\", # \"palm\", \"openai\", or \"local\"\n",
    "                                        min_cluster_size = 5,\n",
    "                                        min_samples = 2,\n",
    "                                        use_llm_cluster_descriptions = True,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "df.to_csv(\"cais_data_taxonomy.csv\", index=False)\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a GSC Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy\n",
    "\n",
    "brand = \"Green Group\"\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(\"sc-domain:greengroupcompanies.com\",\n",
    "                                        text_column = None,\n",
    "                                        search_volume_column = None,\n",
    "                                        platform = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        days = 30,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 2,\n",
    "                                        brand = None,\n",
    "                                        limit_queries = 5)\n",
    "\n",
    "df = add_categories(taxonomy, df, brand) \n",
    "\n",
    "df.to_csv(\"greengroupcompanies_taxonomy.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
