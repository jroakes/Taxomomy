{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ngrams based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy\n",
    "\n",
    "filename = \"cais_data_expanded.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = False,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "df.to_csv(\"cais_data_expanded_ngram_taxonomy.csv\", index=False)\n",
    "\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Description based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LLM descriptions of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy, add_categories\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = True,\n",
    "                                        use_llm_cluster_descriptions = True,\n",
    "                                        cluster_embeddings_model = \"openai\", # \"palm\", \"openai\", or \"local\"\n",
    "                                        min_cluster_size = 5,\n",
    "                                        min_samples = 2,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "\n",
    "df.to_csv(\"taxonomy_cluster_llm.csv\", index=False)\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using c-tfidf ngrams for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy, add_categories\n",
    "\n",
    "filename = \"cais_data_expanded.csv\"\n",
    "brand_terms = ['cais', 'cais group', 'glas', 'glas funds', 'halo', 'halo investing', 'icapital', 'icapital network']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = True,\n",
    "                                        use_llm_cluster_descriptions = False,\n",
    "                                        cluster_embeddings_model = \"openai\", # \"palm\", \"openai\", or \"local\"\n",
    "                                        min_cluster_size = 20,\n",
    "                                        min_samples = 3,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 5,\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "df.to_csv(\"cais_data_expanded_taxonomy.csv\", index=False)\n",
    "print(\"\\n\".join(taxonomy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "\u001b[32m2023-07-11 10:33:39.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mget_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mFound URL column: url.\u001b[0m\n",
      "\u001b[32m2023-07-11 10:33:41.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (1862, 2)\u001b[0m\n",
      "\u001b[32m2023-07-11 10:33:41.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mMerged Ngrams. Dataframe shape: (1768, 4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from main import create_taxonomy, get_data, score_and_filter_df\n",
    "\n",
    "filename = \"HM Raw Data.csv\"\n",
    "brand_terms = ['luke', 'lukes', 'md anderson', 'anderson', 'hca', 'stlukes', 'memorial', 'memorial hermann', \n",
    "               'hermann', 'herman', 'houston methodist', 'methodist', 'st joseph', 'joseph']\n",
    "\n",
    "df, df_original = get_data(filename, \n",
    "                           text_column = \"keyword\", \n",
    "                           limit_queries = 5,\n",
    "                           brand_terms = brand_terms,\n",
    "                           search_volume_column = \"search_volume\")\n",
    "\n",
    "df = score_and_filter_df(df, min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "\u001b[32m2023-07-12 13:17:43.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mget_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mFound URL column: url.\u001b[0m\n",
      "\u001b[32m2023-07-12 13:17:45.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (4955, 2)\u001b[0m\n",
      "\u001b[32m2023-07-12 13:17:51.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mMerged Ngrams. Dataframe shape: (4413, 4)\u001b[0m\n",
      "\u001b[32m2023-07-12 13:23:56.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mFinal score and filter length: 4413\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ngram_size     query  merged_frequency  frequency  search_volume     score\n",
      "0           1   houston          1.000000   0.847758       0.077463  0.538732\n",
      "1           1  targeted          0.008805   0.002754       1.000000  0.504403\n",
      "2           1    cancer          0.379874   1.000000       0.411950  0.395912\n",
      "3           1    doctor          0.573585   0.248623       0.010016  0.291800\n",
      "4           1  symptoms          0.335849   0.125885       0.155124  0.245486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram_size</th>\n",
       "      <th>query</th>\n",
       "      <th>merged_frequency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>search_volume</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>houston</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847758</td>\n",
       "      <td>0.077463</td>\n",
       "      <td>0.538732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>targeted</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.504403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.379874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411950</td>\n",
       "      <td>0.395912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>doctor</td>\n",
       "      <td>0.573585</td>\n",
       "      <td>0.248623</td>\n",
       "      <td>0.010016</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>0.125885</td>\n",
       "      <td>0.155124</td>\n",
       "      <td>0.245486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>near</td>\n",
       "      <td>0.276730</td>\n",
       "      <td>0.177026</td>\n",
       "      <td>0.111410</td>\n",
       "      <td>0.194070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>cardiologist</td>\n",
       "      <td>0.384906</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.194023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>memorial</td>\n",
       "      <td>0.296855</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.051773</td>\n",
       "      <td>0.174314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>treatment</td>\n",
       "      <td>0.310692</td>\n",
       "      <td>0.133360</td>\n",
       "      <td>0.017994</td>\n",
       "      <td>0.164343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>texas</td>\n",
       "      <td>0.237736</td>\n",
       "      <td>0.168371</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.138247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>0.252830</td>\n",
       "      <td>0.116050</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.127918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>obgyn</td>\n",
       "      <td>0.231447</td>\n",
       "      <td>0.090087</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.116987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>office</td>\n",
       "      <td>0.228931</td>\n",
       "      <td>0.073564</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.114928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>breast cancer</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.090873</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>0.108233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>hospital</td>\n",
       "      <td>0.167296</td>\n",
       "      <td>0.241542</td>\n",
       "      <td>0.043308</td>\n",
       "      <td>0.105302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>urgent care</td>\n",
       "      <td>0.139623</td>\n",
       "      <td>0.066090</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.105105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.049567</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.104380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>woodlands</td>\n",
       "      <td>0.196226</td>\n",
       "      <td>0.166798</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>0.102404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>neurologist</td>\n",
       "      <td>0.184906</td>\n",
       "      <td>0.086546</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>0.097162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>doctors</td>\n",
       "      <td>0.182390</td>\n",
       "      <td>0.079465</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>0.096791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ngram_size          query  merged_frequency  frequency  search_volume  \\\n",
       "0            1        houston          1.000000   0.847758       0.077463   \n",
       "1            1       targeted          0.008805   0.002754       1.000000   \n",
       "2            1         cancer          0.379874   1.000000       0.411950   \n",
       "3            1         doctor          0.573585   0.248623       0.010016   \n",
       "4            1       symptoms          0.335849   0.125885       0.155124   \n",
       "5            1           near          0.276730   0.177026       0.111410   \n",
       "6            1   cardiologist          0.384906   0.161290       0.003141   \n",
       "7            1       memorial          0.296855   0.834382       0.051773   \n",
       "8            1      treatment          0.310692   0.133360       0.017994   \n",
       "9            1          texas          0.237736   0.168371       0.038757   \n",
       "10           1     cardiology          0.252830   0.116050       0.003005   \n",
       "11           1          obgyn          0.231447   0.090087       0.002528   \n",
       "12           1         office          0.228931   0.073564       0.000925   \n",
       "13           2  breast cancer          0.188679   0.090873       0.027787   \n",
       "14           1       hospital          0.167296   0.241542       0.043308   \n",
       "15           2    urgent care          0.139623   0.066090       0.070588   \n",
       "16           1      hospitals          0.105660   0.049567       0.103100   \n",
       "17           1      woodlands          0.196226   0.166798       0.008581   \n",
       "18           1    neurologist          0.184906   0.086546       0.009418   \n",
       "19           1        doctors          0.182390   0.079465       0.011192   \n",
       "\n",
       "       score  \n",
       "0   0.538732  \n",
       "1   0.504403  \n",
       "2   0.395912  \n",
       "3   0.291800  \n",
       "4   0.245486  \n",
       "5   0.194070  \n",
       "6   0.194023  \n",
       "7   0.174314  \n",
       "8   0.164343  \n",
       "9   0.138247  \n",
       "10  0.127918  \n",
       "11  0.116987  \n",
       "12  0.114928  \n",
       "13  0.108233  \n",
       "14  0.105302  \n",
       "15  0.105105  \n",
       "16  0.104380  \n",
       "17  0.102404  \n",
       "18  0.097162  \n",
       "19  0.096791  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import create_taxonomy, get_data, score_and_filter_df\n",
    "\n",
    "filename = \"HM Raw Data.csv\"\n",
    "\n",
    "\n",
    "brand_terms = ['luke', 'lukes', 'md anderson', 'anderson', 'hca', 'stlukes', 'memorial hermann', \n",
    "               'hermann', 'herman', 'houston methodist', 'methodist', 'st joseph', 'joseph']\n",
    "\n",
    "\n",
    "df, df_original = get_data(filename,\n",
    "                           text_column = \"keyword\",\n",
    "                           search_volume_column = \"search_volume\",\n",
    "                           limit_queries = 5,\n",
    "                           brand_terms = brand_terms)\n",
    "\n",
    "df = score_and_filter_df(df, min_df=5)\n",
    "\n",
    "df.head(20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1000).to_csv(\"hm_data_ngrams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Services > Cancer Treatment\n",
      "Services > Cancer Treatment > Targeted Therapy\n",
      "Services > Cancer Treatment > Chemotherapy\n",
      "Services > Cancer Treatment > Radiation Therapy\n",
      "Services > Cancer Treatment > Immunotherapy\n",
      "Services > Cancer Treatment > Surgical Oncology\n",
      "Services > Cancer Treatment > Proton Therapy\n",
      "Services > Cancer Treatment > Clinical Trials\n",
      "Services > Diagnostic Services\n",
      "Services > Diagnostic Services > Imaging\n",
      "Services > Diagnostic Services > Biopsy\n",
      "Services > Diagnostic Services > Genetic Testing\n",
      "Services > Diagnostic Services > Pathology\n",
      "Services > Diagnostic Services > Radiology\n",
      "Services > Supportive Care\n",
      "Services > Supportive Care > Pain Management\n",
      "Services > Supportive Care > Palliative Care\n",
      "Services > Supportive Care > Rehabilitation\n",
      "Services > Supportive Care > Nutritional Support\n",
      "Services > Supportive Care > Counseling\n",
      "Services > Supportive Care > Support Groups\n",
      "Conditions > Breast Cancer\n",
      "Conditions > Lung Cancer\n",
      "Conditions > Colorectal Cancer\n",
      "Conditions > Prostate Cancer\n",
      "Conditions > Ovarian Cancer\n",
      "Conditions > Pancreatic Cancer\n",
      "Conditions > Leukemia\n",
      "Conditions > Lymphoma\n",
      "Conditions > Brain Tumor\n",
      "Conditions > Melanoma\n",
      "Conditions > Thyroid Cancer\n",
      "Conditions > Esophageal Cancer\n",
      "Conditions > Stomach Cancer\n",
      "Conditions > Liver Cancer\n",
      "Conditions > Bladder Cancer\n",
      "Conditions > Kidney Cancer\n",
      "Conditions > Cervical Cancer\n",
      "Conditions > Uterine Cancer\n",
      "Conditions > Testicular Cancer\n",
      "Conditions > Bone Cancer\n",
      "Doctors > Find a Doctor\n",
      "Doctors > Oncologists\n",
      "Doctors > Surgeons\n",
      "Doctors > Radiologists\n",
      "Doctors > Pathologists\n",
      "Doctors > Genetic Counselors\n",
      "Doctors > Palliative Care Specialists\n",
      "Locations > Houston\n",
      "Locations > Woodlands\n",
      "Locations > Sugar Land\n",
      "Locations > Katy\n",
      "Locations > Pearland\n",
      "Locations > Baytown\n",
      "Locations > Conroe\n",
      "Locations > Lufkin\n",
      "Locations > Livingston\n",
      "Locations > League City\n",
      "Locations > Webster\n",
      "Patient Resources > Cancer Symptoms\n",
      "Patient Resources > Cancer Prevention\n",
      "Patient Resources > Cancer Screening\n",
      "Patient Resources > Cancer Diagnosis\n",
      "Patient Resources > Treatment Options\n",
      "Patient Resources > Clinical Trials\n",
      "Patient Resources > Patient Portal\n",
      "Patient Resources > Insurance Information\n",
      "Patient Resources > Financial Assistance\n",
      "Patient Resources > Support Services\n",
      "News & Events > Latest News\n",
      "News & Events > Events & Seminars\n",
      "News & Events > Patient Stories\n",
      "Contact Us > Locations & Directions\n",
      "Contact Us > Phone Directory\n",
      "Contact Us > Appointment Request\n",
      "Contact Us > Online Consultation\n",
      "Contact Us > Careers\n"
     ]
    }
   ],
   "source": [
    "from lib.api import get_openai_response_chat\n",
    "from lib.nlp import get_structure\n",
    "\n",
    "subject = \"Taxonomy for Cancer Hospital System Website\"\n",
    "query_data = df.head(1000)[['query', 'score']].to_markdown(index=None)\n",
    "brand_terms = ['luke', 'lukes', 'md anderson', 'anderson', 'hca', 'stlukes', 'memorial hermann', \n",
    "               'hermann', 'herman', 'houston methodist', 'methodist', 'st joseph', 'joseph']\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "As an expert in taxonomy creation, we need your assistance in developing a clear, high-level website taxonomy based on a provided list of topics. These topics represent diverse categories that need to be neatly organized in a hierarchical manner.\n",
    "\n",
    "Subject: {subject}\n",
    "\n",
    "Topics:\n",
    "{query_data}\n",
    "\n",
    "The topics are a list of topic ngrams and their scores. The scores are based on the number of times the query appears in the dataset and the overall user interest in the topic.  Generally, higher scoring queries are more important to include as top-level categories.\n",
    "\n",
    "Please adhere to the following format for your output:\n",
    "\n",
    "- Category\n",
    "  - Subcategory\n",
    "    - Sub-subcategory\n",
    "  - Subcategory\n",
    "- Category\n",
    "  - Subcategory\n",
    "  ...\n",
    "\n",
    "If anything can't be categorized, please add it to the Miscellaneous category. Please exclude mentioning the following brand terms in the taxonomy: {brand_terms}.  In addition, ignore any topics that are not relevant to the products and services offered by the company.\n",
    "\n",
    "Begin!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = get_openai_response_chat(prompt, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "taxonomy = get_structure(response)\n",
    "\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = ['Services > Cancer Treatment',\n",
    " 'Services > Cancer Treatment > Targeted Therapy',\n",
    " 'Services > Cancer Treatment > Chemotherapy',\n",
    " 'Services > Cancer Treatment > Radiation Therapy',\n",
    " 'Services > Cancer Treatment > Immunotherapy',\n",
    " 'Services > Cancer Treatment > Surgical Oncology',\n",
    " 'Services > Cancer Treatment > Proton Therapy',\n",
    " 'Services > Cancer Treatment > Clinical Trials',\n",
    " 'Services > Diagnostic Services',\n",
    " 'Services > Diagnostic Services > Imaging',\n",
    " 'Services > Diagnostic Services > Biopsy',\n",
    " 'Services > Diagnostic Services > Genetic Testing',\n",
    " 'Services > Diagnostic Services > Pathology',\n",
    " 'Services > Diagnostic Services > Radiology',\n",
    " 'Services > Supportive Care',\n",
    " 'Services > Supportive Care > Pain Management',\n",
    " 'Services > Supportive Care > Palliative Care',\n",
    " 'Services > Supportive Care > Rehabilitation',\n",
    " 'Services > Supportive Care > Nutritional Support',\n",
    " 'Services > Supportive Care > Counseling',\n",
    " 'Services > Supportive Care > Support Groups',\n",
    " 'Conditions > Breast Cancer',\n",
    " 'Conditions > Lung Cancer',\n",
    " 'Conditions > Colorectal Cancer',\n",
    " 'Conditions > Prostate Cancer',\n",
    " 'Conditions > Ovarian Cancer',\n",
    " 'Conditions > Pancreatic Cancer',\n",
    " 'Conditions > Leukemia',\n",
    " 'Conditions > Lymphoma',\n",
    " 'Conditions > Brain Tumor',\n",
    " 'Conditions > Melanoma',\n",
    " 'Conditions > Thyroid Cancer',\n",
    " 'Conditions > Esophageal Cancer',\n",
    " 'Conditions > Stomach Cancer',\n",
    " 'Conditions > Liver Cancer',\n",
    " 'Conditions > Bladder Cancer',\n",
    " 'Conditions > Kidney Cancer',\n",
    " 'Conditions > Cervical Cancer',\n",
    " 'Conditions > Uterine Cancer',\n",
    " 'Conditions > Testicular Cancer',\n",
    " 'Conditions > Bone Cancer',\n",
    " 'Doctors > Find a Doctor',\n",
    " 'Doctors > Oncologists',\n",
    " 'Doctors > Surgeons',\n",
    " 'Doctors > Radiologists',\n",
    " 'Doctors > Pathologists',\n",
    " 'Doctors > Genetic Counselors',\n",
    " 'Doctors > Palliative Care Specialists',\n",
    " 'Locations > Houston',\n",
    " 'Locations > Woodlands',\n",
    " 'Locations > Sugar Land',\n",
    " 'Locations > Katy',\n",
    " 'Locations > Pearland',\n",
    " 'Locations > Baytown',\n",
    " 'Locations > Conroe',\n",
    " 'Locations > Lufkin',\n",
    " 'Locations > Livingston',\n",
    " 'Locations > League City',\n",
    " 'Locations > Webster',\n",
    " 'Patient Resources > Cancer Symptoms',\n",
    " 'Patient Resources > Cancer Prevention',\n",
    " 'Patient Resources > Cancer Screening',\n",
    " 'Patient Resources > Cancer Diagnosis',\n",
    " 'Patient Resources > Treatment Options',\n",
    " 'Patient Resources > Clinical Trials',\n",
    " 'Patient Resources > Patient Portal',\n",
    " 'Patient Resources > Insurance Information',\n",
    " 'Patient Resources > Financial Assistance',\n",
    " 'Patient Resources > Support Services',\n",
    " 'News & Events > Latest News',\n",
    " 'News & Events > Events & Seminars',\n",
    " 'News & Events > Patient Stories',\n",
    " 'Contact Us > Locations & Directions',\n",
    " 'Contact Us > Phone Directory',\n",
    " 'Contact Us > Appointment Request',\n",
    " 'Contact Us > Online Consultation',\n",
    " 'Contact Us > Careers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-12 13:41:12.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "\u001b[32m2023-07-12 13:41:14.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mGetting embeddings.\u001b[0m\n",
      "\u001b[32m2023-07-12 13:41:14.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "Batches: 100%|██████████| 1862/1862 [10:53<00:00,  2.85it/s]\n",
      "\u001b[32m2023-07-12 13:52:11.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_cluster_model\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mCluster Model: hdbscan\u001b[0m\n",
      "\u001b[32m2023-07-12 13:56:01.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_elbow\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mUsing epsilon value: 2.0350298657256932e-08\u001b[0m\n",
      "\u001b[32m2023-07-12 13:56:01.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mFitting model.\u001b[0m\n",
      "\u001b[32m2023-07-12 13:56:01.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_reduced\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mReducing embeddings to 2 dims\u001b[0m\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "\u001b[32m2023-07-12 14:00:22.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mInitial Model. Unique Labels: 2389\u001b[0m\n",
      "\u001b[32m2023-07-12 14:00:22.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1mRunning post processes for outliers.\u001b[0m\n",
      "\u001b[32m2023-07-12 14:00:38.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m452\u001b[0m - \u001b[1mPost Processing. Unique Labels: 2389\u001b[0m\n",
      "\u001b[32m2023-07-12 14:00:38.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m456\u001b[0m - \u001b[1mFinding names for cluster labels.\u001b[0m\n",
      "\u001b[32m2023-07-12 14:06:46.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from main import add_categories_clustered\n",
    "\n",
    "df = add_categories_clustered(taxonomy, df_original, \n",
    "                             cluster_embeddings_model = None,\n",
    "                             min_cluster_size = 20,\n",
    "                             min_samples = 5,\n",
    "                             cluster_model = \"hdbscan\",\n",
    "                             match_col = \"query\")\n",
    "\n",
    "df.to_csv(\"hm_data_clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3723/3723 [05:01<00:00, 12.34it/s]\n",
      "2023-07-12 09:20:25,847 - BERTopic - Transformed documents to Embeddings\n",
      "2023-07-12 09:23:43,662 - BERTopic - Reduced dimensionality\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "model = BERTopic(language=\"english\", verbose=True, calculate_probabilities=True )\n",
    "\n",
    "topics, probs = model.fit_transform(df[\"query\"].values)\n",
    "\n",
    "model.get_topic_freq().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mget_topic_info()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hierarchical_topics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mhierarchical_topics(df_original[\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m      2\u001b[0m tree \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_topic_tree(hierarchical_topics)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(tree)\n",
      "File \u001b[1;32mc:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\bertopic\\_bertopic.py:945\u001b[0m, in \u001b[0;36mBERTopic.hierarchical_topics\u001b[1;34m(self, docs, linkage_function, distance_function)\u001b[0m\n\u001b[0;32m    942\u001b[0m Z \u001b[39m=\u001b[39m linkage_function(X)\n\u001b[0;32m    944\u001b[0m \u001b[39m# Calculate basic bag-of-words to be iteratively merged later\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m documents \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m\"\u001b[39;49m\u001b[39mDocument\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs,\n\u001b[0;32m    946\u001b[0m                           \u001b[39m\"\u001b[39;49m\u001b[39mID\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(docs)),\n\u001b[0;32m    947\u001b[0m                           \u001b[39m\"\u001b[39;49m\u001b[39mTopic\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtopics_})\n\u001b[0;32m    948\u001b[0m documents_per_topic \u001b[39m=\u001b[39m documents\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m], as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin})\n\u001b[0;32m    949\u001b[0m documents_per_topic \u001b[39m=\u001b[39m documents_per_topic\u001b[39m.\u001b[39mloc[documents_per_topic\u001b[39m.\u001b[39mTopic \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = model.hierarchical_topics(df_original[\"query\"].values)\n",
    "tree = model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "\u001b[32m2023-07-11 10:59:13.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mget_data\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mFound URL column: url.\u001b[0m\n",
      "\u001b[32m2023-07-11 10:59:14.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mGot Data. Dataframe shape: (28773, 4)\u001b[0m\n",
      "\u001b[32m2023-07-11 10:59:14.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m257\u001b[0m - \u001b[1mUsing Elbow to define top ngram queries.\u001b[0m\n",
      "\u001b[32m2023-07-11 10:59:15.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (1386, 2)\u001b[0m\n",
      "\u001b[32m2023-07-11 10:59:15.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mscore_and_filter_df\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mMerged Ngrams. Dataframe shape: (1307, 4)\u001b[0m\n",
      "\u001b[32m2023-07-11 11:00:32.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mGot ngram frequency. Dataframe shape: (1307, 6)\u001b[0m\n",
      "\u001b[32m2023-07-11 11:00:32.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m261\u001b[0m - \u001b[1mGot samples. Number of samples: 500\u001b[0m\n",
      "\u001b[32m2023-07-11 11:00:33.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mUsing OpenAI API.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:08.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mReviewing OpenAI's work.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:45.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1mGetting structure.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:45.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mcreate_taxonomy\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mAdding categories.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:45.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:45.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m434\u001b[0m - \u001b[1mGetting embeddings.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:01:45.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_embeddings\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing local embeddings\u001b[0m\n",
      "Batches: 100%|██████████| 1862/1862 [09:49<00:00,  3.16it/s]\n",
      "\u001b[32m2023-07-11 11:11:38.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_cluster_model\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mCluster Model: agglomerative\u001b[0m\n",
      "\u001b[32m2023-07-11 11:11:39.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mfit\u001b[0m:\u001b[36m440\u001b[0m - \u001b[1mFitting model.\u001b[0m\n",
      "\u001b[32m2023-07-11 11:11:39.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlib.clustering\u001b[0m:\u001b[36mget_reduced\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mReducing embeddings to 5 dims\u001b[0m\n",
      "c:\\Users\\jroak\\anaconda3\\envs\\taxonomy\\lib\\site-packages\\umap\\spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from main import create_taxonomy\n",
    "\n",
    "filename = \"HM Raw Data.csv\"\n",
    "\n",
    "\n",
    "brand_terms = ['luke', 'lukes', 'md anderson', 'anderson', 'hca', 'stlukes', 'memorial hermann', \n",
    "               'hermann', 'herman', 'houston methodist', 'methodist', 'st joseph', 'joseph']\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(filename,\n",
    "                                        text_column = \"keyword\",\n",
    "                                        search_volume_column = \"search_volume\",\n",
    "                                        taxonomy_model = \"openai\", # \"palm\", \"openai\"\n",
    "                                        match_back_type = \"cluster\",\n",
    "                                        match_back_cluster_model = \"agglomerative\",\n",
    "                                        use_clustering = False,\n",
    "                                        min_cluster_size = 10,\n",
    "                                        min_samples = 3,\n",
    "                                        limit_queries = 3,\n",
    "                                        ngram_range = (1, 5),\n",
    "                                        min_df = 10,\n",
    "\n",
    "                                        brand_terms = brand_terms)\n",
    "\n",
    "\n",
    "df.to_csv(\"HM_Raw_Data_ngram_taxonomy2.csv\", index=False)\n",
    "print(\"\\n\".join(taxonomy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39;49mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy, add_categories_clustered, add_categories\n",
    "import pandas as pd\n",
    "\n",
    "taxonomy = \"\"\"Alternative Investments > Alternative Assets\n",
    "Alternative Investments > Annuities\n",
    "Alternative Investments > Annuities > AIG Annuities\n",
    "Alternative Investments > Annuities > Jackson Annuities\n",
    "Alternative Investments > Annuities > Multi Year Guaranteed Annuity\n",
    "Alternative Investments > Distressed\n",
    "Alternative Investments > Distressed > Distressed Debt\n",
    "Alternative Investments > Distressed > Distressed Investing\n",
    "Alternative Investments > ETFs\n",
    "Alternative Investments > ETFs > Buffer ETFs\n",
    "Alternative Investments > ETFs > Defined Outcome ETFs\n",
    "Alternative Investments > Hedge Funds\n",
    "Alternative Investments > Hedge Funds > Distressed Hedge Funds\n",
    "Alternative Investments > Hedge Funds > Global Macro Hedge Fund\n",
    "Alternative Investments > Hedge Funds > Hedged Equity\n",
    "Alternative Investments > Private Equity\n",
    "Alternative Investments > Private Equity > Distressed Private Equity\n",
    "Alternative Investments > Private Equity > Evergreen Private Equity\n",
    "Alternative Investments > Private Equity > Secondary Private Equity\n",
    "Alternative Investments > Real Estate > Private Real Estate Market\n",
    "Alternative Investments > Real Estate > Unlisted REIT\n",
    "Alternative Investments > Strategies\n",
    "Alternative Investments > Structured Products\n",
    "Alternative Investments > Structured Products > Structured Equity Products\n",
    "Alternative Investments > Structured Products > Structured ETFs\n",
    "Alternative Investments > Structured Products > Structured Notes\n",
    "Events > Alternative Investment Summit\n",
    "Financial Services > Broker Dealer\n",
    "Financial Services > Financial Advisors\n",
    "Financial Services > Financing\n",
    "Financial Services > Investment Platforms\n",
    "Financial Services > Investment Platforms > Alternative Investment Platforms\n",
    "Financial Services > Investment Platforms > Independent Financial Advisor Platforms\n",
    "Financial Services > Investment Platforms > White Label Investment Platform\n",
    "Investing > Capital\n",
    "Investing > Capital > BDC Capital\n",
    "Investing > Capital > Capital REIT\n",
    "Investing > Capital > Private Capital\n",
    "Investing > Funds\n",
    "Investing > Funds > Alternative Investment Funds\n",
    "Investing > Funds > Hedge Funds\n",
    "Investing > Funds > Interval Fund\n",
    "Investing > Funds > Investment REITs\n",
    "Investing > Funds > Private Credit Fund\n",
    "Investing > Funds > Private Equity Funds\n",
    "Investing > Private Equity\n",
    "Investing > Private Equity > Capital\n",
    "Investing > Private Equity > Equity\n",
    "Investing > Private Equity > Firms\n",
    "Investing > Private Equity > Funds\n",
    "Investing > Real Estate \n",
    "Investing > Real Estate > Trusts\n",
    "Investing > Real Estate > Funds\n",
    "Investing > Strategies\n",
    "Investing > Strategies > 60 20 20 portfolio\n",
    "Investing > Strategies > 60 40\n",
    "Investing > Strategies > 60 40 simplified\n",
    "Investing > Strategies > Returns\n",
    "Investing > Strategies > Skew Negative\n",
    "Investing > Strategies > Skew Positive\n",
    "Regulations\n",
    "Regulations > Reg BI\n",
    "Technology\n",
    "Technology > Alternative Investment\n",
    "Technolgy > Software\n",
    "Technology > Platform APIs\n",
    "Technology > Fintech\n",
    "Miscellaneous\n",
    "Miscellaneous > Person\n",
    "Miscellaneous > Company\n",
    "Miscellaneous > Log in\n",
    "Miscellaneous > Sign up\n",
    "Miscellaneous > Contact\n",
    "Miscellaneous > Location\n",
    "Miscellaneous > CAIS\n",
    "Miscellaneous > Forge\n",
    "Miscellaneous > Halo\n",
    "Miscellaneous > iCapital\n",
    "Other\"\"\".split(\"\\n\")\n",
    "\n",
    "taxonomy = [t.strip() for t in taxonomy]\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "df_result = add_categories_clustered(taxonomy, df, \n",
    "                         cluster_embeddings_model = \"openai\",\n",
    "                         min_cluster_size = 3,\n",
    "                         min_samples = 1,\n",
    "                         cluster_model = \"agglomerative\",\n",
    "                         match_col = \"keyword\")\n",
    "\n",
    "\n",
    "df_result.to_csv(\"cais_data_taxonomy_final_agg2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert classifier, we need your help to classify the keywords into the given categories.\n",
      "\n",
      "Categories:\n",
      "Alternative Assets\n",
      "Annuities\n",
      "AIG Annuities\n",
      "Jackson Annuities\n",
      "Multi Year Guaranteed Annuity\n",
      "Distressed\n",
      "Distressed Debt\n",
      "Distressed Investing\n",
      "ETFs\n",
      "Buffer ETFs\n",
      "Defined Outcome ETFs\n",
      "Hedge Funds\n",
      "Distressed Hedge Funds\n",
      "Global Macro Hedge Fund\n",
      "Hedged Equity\n",
      "Private Equity\n",
      "Distressed Private Equity\n",
      "Evergreen Private Equity\n",
      "Secondary Private Equity\n",
      "Real Estate\n",
      "Private Real Estate Market\n",
      "Unlisted REIT\n",
      "Strategies\n",
      "Structured Products\n",
      "Structured Equity Products\n",
      "Structured ETFs\n",
      "Structured Notes\n",
      "Alternative Investment Summit\n",
      "Broker Dealer\n",
      "Financial Advisors\n",
      "Financing\n",
      "Investment Platforms\n",
      "Alternative Investment Platforms\n",
      "Independent Financial Advisor Platforms\n",
      "White Label Investment Platform\n",
      "Capital\n",
      "BDC Capital\n",
      "Capital REIT\n",
      "Private Capital\n",
      "Funds\n",
      "Alternative Investment Funds\n",
      "Funds Hedge Funds\n",
      "Interval Fund\n",
      "Investment REITs\n",
      "Private Credit Fund\n",
      "Private Equity Funds\n",
      "Investing Private Equity\n",
      "Private Equity Capital\n",
      "Equity\n",
      "Firms\n",
      "Private Equity Funds\n",
      "Real Estate \n",
      "Trusts\n",
      "Real Estate Funds\n",
      "Investing Strategies\n",
      "60 20 20 portfolio\n",
      "60 40\n",
      "60 40 simplified\n",
      "Returns\n",
      "Skew Negative\n",
      "Skew Positive\n",
      "Regulations\n",
      "Reg BI\n",
      "Technology\n",
      "Alternative Investment\n",
      "Software\n",
      "Platform APIs\n",
      "Fintech\n",
      "Miscellaneous\n",
      "Person\n",
      "Company\n",
      "Log in\n",
      "Sign up\n",
      "Contact\n",
      "Location\n",
      "CAIS\n",
      "Forge\n",
      "Halo\n",
      "iCapital\n",
      "Other\n",
      "\n",
      "Keywords:\n",
      "oid insurance\n",
      "addepar cost\n",
      "andrew gosden apollo\n",
      "aig annuity agent login\n",
      "rockefeller investment management\n",
      "structured notes investment\n",
      "negative vs positive skew\n",
      "structured product investments\n",
      "alternative careers for financial advisors\n",
      "private equity multiples\n",
      "halo api\n",
      "what is a secondaries fund\n",
      "inflation tomorrow\n",
      "www mercer com\n",
      "fund platform\n",
      "s   p notes\n",
      "bb   t securities llc\n",
      "portfolio rate annuities\n",
      "charles schwab alternative investments\n",
      "portfolio investment solutions\n",
      "ares new york office\n",
      "mercer advisors careers\n",
      "alternatives to fidelity\n",
      "buffered etf risks\n",
      "portfolio company value creation\n",
      "logo halo\n",
      "los angeles alternative investment conference\n",
      "bluerock wealth management\n",
      "keebeck wealth management\n",
      "fidelity investments private client group\n",
      "private equity multiples 2022\n",
      "what is a structured note\n",
      "sec reg bi care obligation\n",
      "alumni ventures sec\n",
      "key private bank\n",
      "structure notes\n",
      "cais conference 2023\n",
      "reg bi components\n",
      "finra broker dealer for sale\n",
      "open end real estate fund\n",
      "women with cais\n",
      "icapital com\n",
      "how do structured notes work\n",
      "capital systems\n",
      "multi year guaranteed annuity rates\n",
      "security benefit eldridge\n",
      "alternative investments\n",
      "www pershing com netx360\n",
      "reit valuations\n",
      "evanston capital management\n",
      "jackson national advisor login\n",
      "monroe capital\n",
      "case alternative investments\n",
      "distressed private equity funds\n",
      "income structured notes\n",
      "ted koenig\n",
      "cais insurance\n",
      "alternatives online access\n",
      "top quartile private equity funds\n",
      "orion investment management\n",
      "investing in notes\n",
      "bridge annuity\n",
      "glass alternative\n",
      "income note\n",
      "ares wealth management\n",
      "jefferies asset management\n",
      "morningstar investment conference\n",
      "alternative investment education\n",
      "pershing a bny mellon company\n",
      "cais aum\n",
      "halo tech group\n",
      "fidelity portfolio visualizer\n",
      "non traded reits performance\n",
      "private equity secondary funds\n",
      "investing in distressed companies\n",
      "access investing\n",
      "distressed debt firms\n",
      "what is distressed credit\n",
      "finra cais reporting\n",
      "cais finra\n",
      "investing com\n",
      "guggenheim investment\n",
      "why 60 40 portfolio\n",
      "unlisted reit\n",
      "stages of investing\n",
      "private equity value creation\n",
      "simplified alternatives\n",
      "broker dealer finop\n",
      "investor pershing\n",
      "rockefeller capital\n",
      "alternative financial solutions\n",
      "pe fund performance\n",
      "aig annuities advisor login\n",
      "200 west jackson street\n",
      "investing api\n",
      "protected notes\n",
      "wealth management alternative investments\n",
      "best structured notes\n",
      "focus financial partners investor relations\n",
      "evergreen funding\n",
      "\n",
      "Please classify the keywords into the given categories. If you think a keyword does not belong to any of the categories, please select \"Miscellaneous\" option. Respond ONLY with the classification for each keyword on a separate line.  DO NOT include the keyword, we will match the keyword to the classification later.\n",
      "\n",
      "Begin!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:37, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Miscellaneous\\nMiscellaneous\\nPerson\\nAIG Annuities\\nCompany\\nStructured Notes\\nReturns\\nStructured Products\\nAlternative Assets\\nPrivate Equity\\nTechnology\\nSecondary Private Equity\\nMiscellaneous\\nCompany\\nInvestment Platforms\\nMiscellaneous\\nCompany\\nAnnuities\\nAlternative Assets\\nInvestment Platforms\\nCompany\\nCompany\\nAlternative Assets\\nBuffer ETFs\\nPrivate Equity\\nMiscellaneous\\nAlternative Investment Summit\\nCompany\\nCompany\\nCompany\\nPrivate Equity\\nStructured Notes\\nRegulations\\nCompany\\nCompany\\nStructured Notes\\nAlternative Investment Summit\\nRegulations\\nBroker Dealer\\nReal Estate\\nMiscellaneous\\nCompany\\nMiscellaneous\\nReal Estate\\nCompany\\nStructured Notes\\nCapital\\nMulti Year Guaranteed Annuity\\nCompany\\nAlternative Assets\\nTechnology\\nReal Estate\\nCompany\\nAIG Annuities\\nCompany\\nCompany\\nAlternative Assets\\nDistressed Private Equity\\nStructured Notes\\nPerson\\nMiscellaneous\\nAlternative Assets\\nInvestment Platforms\\nPrivate Equity Funds\\nCompany\\nInvesting Strategies\\nAnnuities\\nMiscellaneous\\nStructured Notes\\nCompany\\nCompany\\nAlternative Investment Summit\\nAlternative Assets\\nCompany\\nMiscellaneous\\nTechnology\\nReal Estate\\nCompany\\nJackson Annuities\\nCompany\\nAlternative Assets\\nDistressed Private Equity\\nStructured Notes\\nPerson\\nMiscellaneous\\nAlternative Assets\\nAlternative Assets\\nPrivate Equity Funds\\nCompany\\nInvesting Strategies\\nAnnuities\\nMiscellaneous\\nCompany\\nCompany\\nMiscellaneous\\nCompany\\nMiscellaneous\\nInvesting Strategies\\nCompany\\nInvesting Strategies\\nUnlisted REIT\\nInvesting Strategies\\nPrivate Equity\\nAlternative Assets\\nBroker Dealer\\nCompany\\nCompany\\nAlternative Assets\\nPrivate Equity\\nAIG Annuities\\nCompany\\nTechnology\\nStructured Notes\\nAlternative Assets\\nStructured Notes\\nCompany\\nCompany\\nPrivate Equity\\nInvesting in Distressed Companies\\nAlternative Assets\\nDistressed Debt\\nMiscellaneous\\nMiscellaneous\\nMiscellaneous\\nCompany\\nInvesting Strategies\\nUnlisted REIT\\nInvesting Strategies\\nPrivate Equity\\nAlternative Assets\\nBroker Dealer\\nCompany\\nCompany\\nAlternative Assets\\nPrivate Equity\\nAIG Annuities\\nCompany\\nTechnology\\nStructured Notes\\nAlternative Assets\\nStructured Notes\\nCompany\\nCompany\\nMiscellaneous'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.api import get_openai_response_chat\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "taxonomy = \"\"\"Alternative Investments > Alternative Assets\n",
    "Alternative Investments > Annuities\n",
    "Alternative Investments > Annuities > AIG Annuities\n",
    "Alternative Investments > Annuities > Jackson Annuities\n",
    "Alternative Investments > Annuities > Multi Year Guaranteed Annuity\n",
    "Alternative Investments > Distressed\n",
    "Alternative Investments > Distressed > Distressed Debt\n",
    "Alternative Investments > Distressed > Distressed Investing\n",
    "Alternative Investments > ETFs\n",
    "Alternative Investments > ETFs > Buffer ETFs\n",
    "Alternative Investments > ETFs > Defined Outcome ETFs\n",
    "Alternative Investments > Hedge Funds\n",
    "Alternative Investments > Hedge Funds > Distressed Hedge Funds\n",
    "Alternative Investments > Hedge Funds > Global Macro Hedge Fund\n",
    "Alternative Investments > Hedge Funds > Hedged Equity\n",
    "Alternative Investments > Private Equity\n",
    "Alternative Investments > Private Equity > Distressed Private Equity\n",
    "Alternative Investments > Private Equity > Evergreen Private Equity\n",
    "Alternative Investments > Private Equity > Secondary Private Equity\n",
    "Alternative Investments > Real Estate\n",
    "Alternative Investments > Real Estate > Private Real Estate Market\n",
    "Alternative Investments > Real Estate > Unlisted REIT\n",
    "Alternative Investments > Strategies\n",
    "Alternative Investments > Structured Products\n",
    "Alternative Investments > Structured Products > Structured Equity Products\n",
    "Alternative Investments > Structured Products > Structured ETFs\n",
    "Alternative Investments > Structured Products > Structured Notes\n",
    "Events > Alternative Investment Summit\n",
    "Financial Services > Broker Dealer\n",
    "Financial Services > Financial Advisors\n",
    "Financial Services > Financing\n",
    "Financial Services > Investment Platforms\n",
    "Financial Services > Investment Platforms > Alternative Investment Platforms\n",
    "Financial Services > Investment Platforms > Independent Financial Advisor Platforms\n",
    "Financial Services > Investment Platforms > White Label Investment Platform\n",
    "Investing > Capital\n",
    "Investing > Capital > BDC Capital\n",
    "Investing > Capital > Capital REIT\n",
    "Investing > Capital > Private Capital\n",
    "Investing > Funds\n",
    "Investing > Funds > Alternative Investment Funds\n",
    "Investing > Funds > Hedge Funds\n",
    "Investing > Funds > Interval Fund\n",
    "Investing > Funds > Investment REITs\n",
    "Investing > Funds > Private Credit Fund\n",
    "Investing > Funds > Private Equity Funds\n",
    "Investing > Private Equity\n",
    "Investing > Private Equity > Capital\n",
    "Investing > Private Equity > Equity\n",
    "Investing > Private Equity > Firms\n",
    "Investing > Private Equity > Funds\n",
    "Investing > Real Estate \n",
    "Investing > Real Estate > Trusts\n",
    "Investing > Real Estate > Funds\n",
    "Investing > Strategies\n",
    "Investing > Strategies > 60 20 20 portfolio\n",
    "Investing > Strategies > 60 40\n",
    "Investing > Strategies > 60 40 simplified\n",
    "Investing > Strategies > Returns\n",
    "Investing > Strategies > Skew Negative\n",
    "Investing > Strategies > Skew Positive\n",
    "Regulations\n",
    "Regulations > Reg BI\n",
    "Technology\n",
    "Technology > Alternative Investment\n",
    "Technol0gy > Software\n",
    "Technology > Platform APIs\n",
    "Technology > Fintech\n",
    "Miscellaneous\n",
    "Miscellaneous > Person\n",
    "Miscellaneous > Company\n",
    "Miscellaneous > Log in\n",
    "Miscellaneous > Sign up\n",
    "Miscellaneous > Contact\n",
    "Miscellaneous > Location\n",
    "Miscellaneous > CAIS\n",
    "Miscellaneous > Forge\n",
    "Miscellaneous > Halo\n",
    "Miscellaneous > iCapital\n",
    "Other\"\"\".split(\"\\n\")\n",
    "\n",
    "filename = \"cais_data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "structure = taxonomy.copy()\n",
    "\n",
    "structure_parts = []\n",
    "for s in structure:\n",
    "        last_parts = s.split(\" > \")[-2:]\n",
    "\n",
    "        if last_parts[-1] not in structure_parts:\n",
    "            structure_parts.append(last_parts[-1])\n",
    "        else:\n",
    "            structure_parts.append(\" \".join(last_parts))\n",
    "\n",
    "structure_map = {p:s for p, s in zip(structure_parts, structure)}\n",
    "\n",
    "structure_map\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"As an expert classifier, we need your help to classify the keywords into the given categories.\n",
    "\n",
    "Categories:\n",
    "{categories}\n",
    "\n",
    "Keywords:\n",
    "{keywords}\n",
    "\n",
    "Please classify the keywords into the given categories. If you think a keyword does not belong to any of the categories, please select \"Miscellaneous\" option. Respond ONLY with the classification for each keyword on a separate line.  DO NOT include the keyword, we will match the keyword to the classification later.\n",
    "\n",
    "Begin!\"\"\"\n",
    "\n",
    "categories = \"\\n\".join(structure_parts)\n",
    "keywords = list(set(df_result[\"keyword\"].tolist()))\n",
    "\n",
    "def batchify(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for batch in tqdm(batchify(keywords, batch_size)):\n",
    "    keyword_str = \"\\n\".join(batch)\n",
    "    prompt = PROMPT.format(categories=categories, keywords=keyword_str)\n",
    "    print(prompt)\n",
    "    response = get_openai_response_chat(prompt, system_message = \"You are an expert keyword classifier.\")\n",
    "    print('Got response')\n",
    "    break\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a GSC Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import create_taxonomy\n",
    "\n",
    "brand_terms = [\"Green Group\", \"greengroup\"]\n",
    "\n",
    "taxonomy, df, samples = create_taxonomy(\"sc-domain:greengroupcompanies.com\",\n",
    "                                        text_column = None,\n",
    "                                        search_volume_column = None,\n",
    "                                        taxonomy_model = \"openai\", # \"palm\" or \"openai\"\n",
    "                                        use_clustering = True,\n",
    "                                        days = 30,\n",
    "                                        ngram_range = (1, 6),\n",
    "                                        min_df = 2,\n",
    "                                        brand_terms = brand_terms,\n",
    "                                        limit_queries = 5)\n",
    "\n",
    "\n",
    "df.to_csv(\"greengroupcompanies_taxonomy.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.api import get_openai_response_chat\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cais_whitespace.csv\")\n",
    "\n",
    "company_description = \"\"\"CAIS Group: Founded in 2009, CAIS, a fintech leader, is transforming the world of alternative investing. For independent financial advisors, CAIS provides access, education, and operational efficiency they can use to transact at scale and capture more wallet share. For asset managers, CAIS can help build their distribution network, capture new business, and cultivate relationships with independent financial advisors\"\"\"\n",
    "\n",
    "PROMPT = \"\"\"As an expert content strategist, please review the following search queries dealing with {taxonomy} and give me back the 1-3 best topic ideas along with the search queries for each topic.\n",
    "\n",
    "Please include topics that would be relevant to the audience of a company with the following descripiton:\n",
    "{description}\n",
    "\n",
    "The search query data is below in the format `<query> - <search volume>\\n`:\n",
    "{data}\n",
    "\n",
    "To do a good job, you should consider the following:\n",
    "* The topic should be relevant to the audience of the company\n",
    "* The topic should be something that the company can write about\n",
    "* The topic MUST be relevant to more than one of the provided queries\n",
    "* The topic keywords supplied in your output MUST be from the provided search queries\n",
    "* If you are not sure about a topic or there are not enough queries, set the topic, Description, and Keywords to `None`\n",
    "\n",
    "Your response should be EXACTLY in the following format with a space between each result:\n",
    "Topic: <topic>\n",
    "Description: <Description of topic - one sentence>\n",
    "Audience: <Audience this topic is relevant to - comma-separated list>\n",
    "Keywords: <keywords this topic covers from the search queries - comma-separated list>\n",
    "\n",
    "Begin!\"\"\"\n",
    "\n",
    "# Lowercase and strip whitespace from queries\n",
    "df[\"query\"] = df[\"query\"].str.lower().str.strip()\n",
    "\n",
    "# Remove commas from search volume\n",
    "df[\"search_volume\"] = df[\"search_volume\"].astype(str).fillna(\"0\")\n",
    "df[\"search_volume\"] = df[\"search_volume\"].str.replace(\",\", \"\")\n",
    "\n",
    "# Fillna on search volume\n",
    "df[\"search_volume\"] = df[\"search_volume\"].fillna(0).astype(int)\n",
    "\n",
    "# Map of query to search volume\n",
    "search_volume_mapping = df.set_index(\"query\")[\"search_volume\"].to_dict()\n",
    "taxonomies = df.groupby(\"taxonomy\").apply(lambda x: \"\\n\".join([f\"{keyword} - {sv}\" for keyword, sv in zip(x[\"query\"].tolist(), x[\"search_volume\"].tolist())])).to_dict()\n",
    "results = []\n",
    "\n",
    "response_template = {'taxonomy': None, 'topic': None, 'description': None, 'audience': None, 'keywords': None, 'search_volume': None}\n",
    "\n",
    "for taxonomy, data in tqdm(taxonomies.items(), total=len(taxonomies)):\n",
    "    resp = get_openai_response_chat(PROMPT.format(taxonomy=taxonomy, description=company_description, data=data))\n",
    "\n",
    "    # Extract the Topic, Description, and Keywords from the response\n",
    "    resp_data = response_template.copy()\n",
    "    resp_data['taxonomy'] = taxonomy\n",
    "\n",
    "    for line in resp.split(\"\\n\"):\n",
    "        if line.startswith(\"Topic:\"):\n",
    "            resp_data['topic'] = line.replace(\"Topic: \", \"\")\n",
    "        elif line.startswith(\"Description:\"):\n",
    "            resp_data['description'] = line.replace(\"Description: \", \"\")\n",
    "        elif line.startswith(\"Audience:\"):\n",
    "            resp_data['audience'] = line.replace(\"Audience: \", \"\")\n",
    "        elif line.startswith(\"Keywords:\"):\n",
    "            keywords = line.replace(\"Keywords: \", \"\")\n",
    "            resp_data['keywords'] = keywords\n",
    "            resp_data['search_volume'] = sum([search_volume_mapping.get(keyword.lower().strip(), 0) for keyword in keywords.split(\",\")])\n",
    "            if resp_data['topic'] != \"None\" and resp_data['search_volume'] > 0:\n",
    "                results.append(resp_data)\n",
    "            resp_data = response_template.copy()\n",
    "            resp_data['taxonomy'] = taxonomy\n",
    "\n",
    "\n",
    "# Create a dataframe from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"cais_taxonomy_whitespace_chat_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
